{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1v3RbPf13uXN",
        "outputId": "c2abccb6-309a-4ac2-c1c4-1ed9be363029"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-SdAcaPn_fYy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"sk-3cc94a27441a472395f00ba59dd1634f\",\n",
        "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
        ")\n",
        "\n",
        "def format_history(chat_history):\n",
        "    \"\"\"è½¬æ¢å†å²è®°å½•ï¼Œè¿‡æ»¤æ€è€ƒå†…å®¹\"\"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "    for msg in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": msg[0]})\n",
        "        if msg[1]:\n",
        "            # æå–æœ€åå›å¤å†…å®¹ï¼ˆè¿‡æ»¤æ€è€ƒå†…å®¹ï¼‰\n",
        "            final_reply = msg[1].split(\"æœ€ç»ˆå›å¤ï¼š\")[-1].strip()\n",
        "            messages.append({\"role\": \"assistant\", \"content\": final_reply})\n",
        "    return messages\n",
        "\n",
        "def predict(message, chat_history):\n",
        "    \"\"\"å¤„ç†åŒ…å«æ€è€ƒå†…å®¹çš„æµå¼å“åº”\"\"\"\n",
        "    chat_history.append((message, \"\"))\n",
        "    messages = format_history(chat_history[:-1])\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen3-235b-a22b\",\n",
        "        messages=messages,\n",
        "        stream=True,\n",
        "        stream_options={\"include_usage\": True},\n",
        "        extra_body={\"enable_thinking\": True},\n",
        "    )\n",
        "\n",
        "    full_response = \"\"\n",
        "    full_reasoning = \"\"\n",
        "    current_reasoning = \"\"\n",
        "    for chunk in completion:\n",
        "        chunk_data = chunk.model_dump()\n",
        "        if not chunk_data.get('choices') or len(chunk_data['choices']) == 0:\n",
        "          continue  # è·³è¿‡æ— æ•ˆçš„chunk\n",
        "        delta = chunk_data['choices'][0]['delta']\n",
        "\n",
        "        # è§£ææ€è€ƒå†…å®¹\n",
        "        if delta.get('reasoning_content'):\n",
        "            current_reasoning = delta['reasoning_content']\n",
        "            full_reasoning += current_reasoning\n",
        "\n",
        "        # è§£ææ­£å¼å›å¤å†…å®¹\n",
        "        if delta.get('content'):\n",
        "            full_response += delta['content']\n",
        "\n",
        "        # æ„å»ºæ˜¾ç¤ºå†…å®¹\n",
        "        display_text = \"\"\n",
        "        if full_reasoning != '':\n",
        "            display_text = \"ğŸ§  æ€è€ƒè¿‡ç¨‹ï¼š\\n\" + full_reasoning\n",
        "        if full_response:\n",
        "            display_text += f\"\\nğŸ’¡ æœ€ç»ˆå›å¤ï¼š\\n{full_response}\"\n",
        "\n",
        "        # æ›´æ–°èŠå¤©è®°å½•\n",
        "        if current_reasoning or delta.get('content'):\n",
        "            chat_history[-1] = (message, display_text)\n",
        "            yield chat_history\n",
        "\n",
        "        # é‡ç½®å½“å‰æ€è€ƒå†…å®¹ï¼ˆé¿å…é‡å¤æ˜¾ç¤ºï¼‰\n",
        "        if delta.get('content'):\n",
        "            current_reasoning = \"\"\n",
        "\n",
        "# è‡ªå®šä¹‰æ ·å¼\n",
        "css = \"\"\"\n",
        ".gr-chatbot {min-height: 600px;}\n",
        ".gr-chatbot .assistant-message {white-space: pre-wrap;}\n",
        ".dark .reasoning {color: #90CAF9;}\n",
        ".reasoning {color: #1E88E5; font-style: italic;}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"# <center>ğŸ¤– åƒé—®å¤§æ¨¡å‹æ™ºèƒ½å¯¹è¯ï¼ˆå«æ€è€ƒè¿‡ç¨‹ï¼‰</center>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(\n",
        "            bubble_full_width=False,\n",
        "            render_markdown=True,\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(\n",
        "            placeholder=\"è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\",\n",
        "            container=False,\n",
        "            scale=5\n",
        "        )\n",
        "        clear = gr.Button(\"ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯\", scale=1)\n",
        "\n",
        "    msg.submit(predict, [msg, chatbot], chatbot)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_name=\"0.0.0.0\",debug=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
